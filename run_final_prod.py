import os
import shutil
import sys
import re
import pandas as pd
import torch
from sentence_transformers import SentenceTransformer
from huggingface_hub import snapshot_download

# ================= 1. å…¨å±€å‚æ•°é…ç½® =================
# æ¨¡å‹ä»“åº“åç§° (å¤šè¯­è¨€ç‰ˆ MiniLM)
MODEL_REPO = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
# æœ¬åœ°ä¿å­˜è·¯å¾„
LOCAL_MODEL_DIR = "./multilingual-minilm-local"

# è¾“å…¥è¾“å‡ºæ–‡ä»¶é…ç½®
TABLE_FILE_IN = "tables_test.csv"
TABLE_FILE_OUT = "tables_result.csv"
COLUMN_FILE_IN = "columns_test.csv"
COLUMN_FILE_OUT = "columns_result.csv"

# æ‰¹å¤„ç†å¤§å° (æ ¹æ®å†…å­˜è°ƒæ•´)
BATCH_SIZE = 512

# ================= 2. è¡Œä¸šä¸“å®¶è¯å…¸ (è§„åˆ™å¼•æ“) =================
# ä½œç”¨ï¼šè‹±æ–‡ç¼©å†™å‘½ä¸­ Key ä¸” ä¸­æ–‡åŒ…å« Value -> å¼ºåˆ¶åŠ  25 åˆ†
COMMON_SYNONYMS = {
    # --- æ ¸å¿ƒæ ‡è¯† ---
    'id':    ['ç¼–å·', 'ä»£ç ', 'æ ‡è¯†', 'åºå·', 'id'],
    'no':    ['ç¼–å·', 'å·ç ', 'åºå·'],
    'code':  ['ç¼–ç ', 'ä»£ç ', 'ç '],
    'num':   ['æ•°é‡', 'æ¬¡æ•°', 'å·'],
    'nm':    ['åç§°', 'å§“å'],
    'name':  ['åç§°', 'å§“å'],
    
    # --- é‡‘é¢ä¸äº¤æ˜“ ---
    'amt':   ['é‡‘é¢', 'è´¹ç”¨', 'é’±'],
    'amount':['é‡‘é¢', 'æ•°é‡'],
    'bal':   ['ä½™é¢', 'å·®é¢'],
    'price': ['ä»·æ ¼', 'å•ä»·'],
    'cost':  ['æˆæœ¬', 'è´¹ç”¨'],
    'rate':  ['åˆ©ç‡', 'æ±‡ç‡', 'æ¯”ä¾‹'],
    'txn':   ['äº¤æ˜“', 'æµæ°´'],
    'trans': ['äº¤æ˜“', 'ä¼ è¾“'],
    'pay':   ['æ”¯ä»˜', 'ä»˜æ¬¾'],
    
    # --- ç»„ç»‡ä¸äººå‘˜ ---
    'org':   ['æœºæ„', 'ç»„ç»‡', 'éƒ¨é—¨'],
    'dept':  ['éƒ¨é—¨', 'ç§‘å®¤'],
    'cust':  ['å®¢æˆ·'],
    'user':  ['ç”¨æˆ·'],
    'emp':   ['å‘˜å·¥', 'äººå‘˜'],
    'mgr':   ['ç»ç†', 'ç®¡ç†'],
    'acct':  ['è´¦æˆ·', 'è´¦å·'],
    
    # --- æ—¶é—´ä¸çŠ¶æ€ ---
    'dt':    ['æ—¥æœŸ'],
    'date':  ['æ—¥æœŸ'],
    'tm':    ['æ—¶é—´'],
    'time':  ['æ—¶é—´', 'æ—¶åˆ†'],
    'ts':    ['æ—¶é—´æˆ³'],
    'stat':  ['çŠ¶æ€', 'æƒ…å†µ'],
    'status':['çŠ¶æ€', 'æƒ…å†µ'],
    'flg':   ['æ ‡å¿—', 'æ ‡è¯†', 'æ˜¯å¦'],
    'flag':  ['æ ‡å¿—', 'æ ‡è¯†', 'æ˜¯å¦'],
    'is':    ['æ˜¯å¦'],
    'curr':  ['å¸ç§', 'å½“å‰'],
    
    # --- é€šç”¨ ---
    'desc':  ['æè¿°', 'è¯´æ˜', 'å¤‡æ³¨'],
    'rem':   ['å¤‡æ³¨', 'æ‘˜è¦'],
    'remark':['å¤‡æ³¨', 'æ‘˜è¦', 'è¯´æ˜'],
    'addr':  ['åœ°å€'],
    'tel':   ['ç”µè¯'],
    'mobile':['æ‰‹æœº', 'ç§»åŠ¨ç”µè¯'],
    'msg':   ['æ¶ˆæ¯', 'ä¿¡æ¯'],
    'err':   ['é”™è¯¯', 'å¼‚å¸¸'],
    'seq':   ['åºå·', 'åºåˆ—']
}

# ================= 3. æ™ºèƒ½ä¸‹è½½ä¸æ ¡éªŒæ¨¡å— =================

def check_model_integrity():
    """
    æ£€æŸ¥æ¨¡å‹æ ¸å¿ƒæ–‡ä»¶æ˜¯å¦å­˜åœ¨ã€‚
    é€‚é… sentence-transformers 2.x ç‰ˆæœ¬ï¼Œå¿…é¡»æ£€æŸ¥ pytorch_model.bin
    """
    # å¿…é¡»åŒ…å«: æƒé‡æ–‡ä»¶, ä¸»é…ç½®, åˆ†è¯é…ç½®
    required_files = ["pytorch_model.bin", "config.json", "tokenizer.json", "sentencepiece.bpe.model"]
    
    if not os.path.exists(LOCAL_MODEL_DIR):
        return False
    
    # ç®€å•çš„å­˜åœ¨æ€§æ£€æŸ¥
    for f in required_files:
        if not os.path.exists(os.path.join(LOCAL_MODEL_DIR, f)):
            # å…¼å®¹æ€§æ£€æŸ¥ï¼šæœ‰äº›æ–‡ä»¶å¯èƒ½åœ¨å­ç›®å½•ï¼Œè¿™é‡Œä¸»è¦æ£€æŸ¥æ ¹ç›®å½•å…³é”®æ–‡ä»¶
            if f == "sentencepiece.bpe.model": continue 
            return False
    return True

def download_model_smartly():
    """
    æ™ºèƒ½ä¸‹è½½é€»è¾‘ (å®˜æ–¹æº + å…¼å®¹æ—§ç‰ˆåº“)ï¼š
    1. è¿æ¥ Hugging Face å®˜æ–¹æœåŠ¡å™¨ã€‚
    2. ä¸‹è½½ *.json (è§£å†³ 1_Pooling/config.json ç¼ºå¤±æŠ¥é”™)ã€‚
    3. ä¸‹è½½ *.model (è§£å†³åˆ†è¯å™¨æŠ¥é”™)ã€‚
    4. ä¸‹è½½ pytorch_model.bin (é€‚é… sentence-transformers 2.2.2)ã€‚
    """
    if check_model_integrity():
        print(f"âœ… æ£€æµ‹åˆ°å®Œæ•´æ¨¡å‹: {LOCAL_MODEL_DIR}ï¼Œè·³è¿‡ä¸‹è½½ã€‚")
        return

    # æ¸…ç†æ®‹æŸç›®å½•
    if os.path.exists(LOCAL_MODEL_DIR):
        print("âš ï¸ æ£€æµ‹åˆ°ç›®å½•ä¸å®Œæ•´ï¼Œæ­£åœ¨æ¸…ç†å¹¶é‡æ–°ä¸‹è½½...")
        try:
            shutil.rmtree(LOCAL_MODEL_DIR)
        except Exception as e:
            print(f"âŒ æ¸…ç†å¤±è´¥: {e} (è¯·æ‰‹åŠ¨åˆ é™¤æ–‡ä»¶å¤¹)")

    print(f"â¬‡ï¸ æ­£åœ¨ä»å®˜æ–¹æºä¸‹è½½æ¨¡å‹: {MODEL_REPO} ...")
    print("   (æ¨¡å¼ï¼šä»…ä¸‹è½½ PyTorch æƒé‡å’Œå¿…è¦é…ç½®ï¼Œçº¦ 470MB)")
    
    try:
        snapshot_download(
            repo_id=MODEL_REPO, 
            local_dir=LOCAL_MODEL_DIR,
            # ã€å…³é”®é…ç½®ã€‘
            # å¿…é¡»åŒ…å« *.json (ä¸ºäº†ä¸‹è½½å­æ–‡ä»¶å¤¹é‡Œçš„é…ç½®)
            # å¿…é¡»åŒ…å« pytorch_model.bin (å…¼å®¹æ€§æœ€ä½³)
            allow_patterns=[
                "*.json", 
                "*.txt", 
                "*.model", 
                "pytorch_model.bin", 
                "README.md"
            ],
            # åšå†³ä¸ä¸‹è½½è¿™äº›å¤§æ–‡ä»¶
            ignore_patterns=["*.safetensors", "*.onnx", "*.h5", "openvino*", "*.msgpack"],
            resume_download=True
        )
        print("âœ… ä¸‹è½½æˆåŠŸï¼")
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {e}")
        print("æç¤ºï¼šå®˜æ–¹æºè¿æ¥è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œæˆ–ä»£ç†è®¾ç½®ã€‚")
        sys.exit(1)

# ================= 4. æ•°æ®å¤„ç†æ ¸å¿ƒé€»è¾‘ =================

def preprocess_text(text):
    """æ–‡æœ¬æ¸…æ´—ï¼šé©¼å³°æ‹†åˆ†ã€å»ç¬¦ã€è½¬å°å†™"""
    if pd.isna(text): return ""
    text = str(text)
    text = text.replace('_', ' ').replace('-', ' ')
    # æ‹†åˆ†é©¼å³° (e.g., 'isDeleted' -> 'is Deleted')
    text = re.sub(r'(?<!^)(?=[A-Z])', ' ', text)
    return text.lower().strip()

def get_synonym_bonus(en_text, cn_text):
    """è®¡ç®—è§„åˆ™å¥–åŠ±åˆ†"""
    en_words = preprocess_text(en_text).split()
    cn_text = str(cn_text)
    
    for word in en_words:
        if word in COMMON_SYNONYMS:
            for cn_keyword in COMMON_SYNONYMS[word]:
                if cn_keyword in cn_text:
                    return 25 # å‘½ä¸­è§„åˆ™ï¼Œå¥–åŠ± 25 åˆ†
    return 0

def process_file(file_in, file_out, type_name, model):
    """é€šç”¨æ–‡ä»¶å¤„ç†æµç¨‹ï¼šè¯»å– -> è®¡ç®— -> ä¿å­˜"""
    if not os.path.exists(file_in):
        print(f"âš ï¸ è·³è¿‡ï¼šæ‰¾ä¸åˆ°è¾“å…¥æ–‡ä»¶ {file_in}")
        return

    print(f"\nğŸš€ æ­£åœ¨å¤„ç† {type_name} ...")
    try:
        df = pd.read_csv(file_in, dtype=str)
    except Exception as e:
        print(f"âŒ è¯»å–æ–‡ä»¶å¤±è´¥: {e}")
        return

    if type_name == 'table':
        col_en, col_cn = 'Table Name', 'è¡¨ä¸­æ–‡å'
    else:
        col_en, col_cn = 'Column Name', 'å­—æ®µä¸­æ–‡å'

    if col_en not in df.columns or col_cn not in df.columns:
        print(f"âŒ åˆ—åé”™è¯¯ï¼šæ–‡ä»¶ä¸­å¿…é¡»åŒ…å« '{col_en}' å’Œ '{col_cn}'")
        return

    # 1. é«˜æ•ˆå»é‡
    print(f"   - åŸå§‹æ•°æ®: {len(df)} è¡Œ")
    df[col_en] = df[col_en].fillna("")
    df[col_cn] = df[col_cn].fillna("")
    
    unique_pairs = df[[col_en, col_cn]].drop_duplicates().reset_index(drop=True)
    print(f"   - å»é‡åéœ€è®¡ç®—: {len(unique_pairs)} è¡Œ")

    # 2. å‘é‡åŒ–
    processed_en = [preprocess_text(x) for x in unique_pairs[col_en]]
    raw_cn = unique_pairs[col_cn].tolist()

    print("   - æ­£åœ¨è®¡ç®— AI è¯­ä¹‰å‘é‡ (CPU)...")
    embeddings_en = model.encode(processed_en, batch_size=BATCH_SIZE, normalize_embeddings=True, show_progress_bar=True)
    embeddings_cn = model.encode(raw_cn, batch_size=BATCH_SIZE, normalize_embeddings=True, show_progress_bar=True)

    # 3. è¯„åˆ†
    print("   - æ­£åœ¨è®¡ç®—ç»¼åˆå¾—åˆ†...")
    tensor_en = torch.tensor(embeddings_en)
    tensor_cn = torch.tensor(embeddings_cn)
    
    cosine_scores = torch.sum(tensor_en * tensor_cn, dim=1)
    base_scores = (torch.clamp(cosine_scores, 0, 1) * 100).int().tolist()

    final_scores = []
    for i, score in enumerate(base_scores):
        en_raw = unique_pairs.iloc[i][col_en]
        cn_raw = unique_pairs.iloc[i][col_cn]
        bonus = get_synonym_bonus(en_raw, cn_raw)
        final_scores.append(min(score + bonus, 100))

    unique_pairs['calc_score'] = final_scores

    # 4. è¿˜åŸä¿å­˜
    if 'å…³è”åº¦' in df.columns:
        df = df.drop(columns=['å…³è”åº¦'])
        
    result_df = pd.merge(df, unique_pairs, on=[col_en, col_cn], how='left')
    result_df = result_df.rename(columns={'calc_score': 'å…³è”åº¦'})
    
    result_df.to_csv(file_out, index=False, encoding='utf-8-sig')
    print(f"âœ… å®Œæˆï¼å·²ä¿å­˜: {file_out}")

# ================= 5. ä¸»ç¨‹åºå…¥å£ =================

def main():
    print("="*50)
    print("      æ•°æ®æ²»ç† AI æ˜ å°„å·¥å…· (å®˜æ–¹æºå…¼å®¹ç‰ˆ)      ")
    print("="*50)

    # 1. ä¸‹è½½/æ£€æŸ¥æ¨¡å‹
    download_model_smartly()
    
    # 2. åŠ è½½æ¨¡å‹
    print(f"\næ­£åœ¨åŠ è½½æ¨¡å‹: {LOCAL_MODEL_DIR} ...")
    try:
        model = SentenceTransformer(LOCAL_MODEL_DIR, device='cpu')
    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        print("å»ºè®®ï¼šåˆ é™¤ç›®å½• ./multilingual-minilm-local åé‡è¯•ã€‚")
        return

    # 3. æ‰§è¡Œä»»åŠ¡
    process_file(TABLE_FILE_IN, TABLE_FILE_OUT, 'table', model)
    process_file(COLUMN_FILE_IN, COLUMN_FILE_OUT, 'column', model)

    print("\nğŸ‰ å…¨éƒ¨ç»“æŸã€‚")

if __name__ == "__main__":
    main()
